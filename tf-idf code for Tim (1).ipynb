{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import math\n",
    "import re\n",
    "\n",
    "from textblob import TextBlob as tb\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def tf(word, blob):\n",
    "    return blob.words.count(word) / len(blob.words)\n",
    "\n",
    "def n_containing(word, bloblist):\n",
    "    return sum(1 for blob in bloblist if word in blob.words)\n",
    "\n",
    "def idf(word, bloblist):\n",
    "    return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))\n",
    "\n",
    "def tfidf(word, blob, bloblist):\n",
    "    return tf(word, blob) * idf(word, bloblist)\n",
    "\n",
    "# list of english stop words\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "# adding new 'stop' words to account for acronyms and other informal texting --> likely would need to be updated specific to Trump's tweets\n",
    "newStopWords = ['lmao', 'lol', 'u', 'ur', 'omg', 'hbu', 'ha', 'haha', 'yo', 'let', 'got', 'im', 'okay', 'alright', 'sure']\n",
    "for newStopWord in newStopWords:\n",
    "    cachedStopWords.append(newStopWord)\n",
    "\n",
    "def removeStopWords(text):\n",
    "    newText = ' '.join([word for word in text.split() if word not in cachedStopWords])\n",
    "    return newText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Trump tweets, each 'document' would be tweets over a given time period. Probably would be best if it were weeks.\n",
    "# so like document1 would be tweets from Aug 4, 2019 - Aug 10, 2019 and document2 would be tweets from Aug 11 - Aug 17\n",
    "document1 = \"Now one of the important tasks is to identify the title in the body, if we analyse the documents, there are different patterns of alignment of title. But most of the titles are centre aligned. Now we need to figure out a way to extract the title. But before we get all pumped up and start coding, let us analyse the dataset little deep.\"\n",
    "document2 = \"Now one of the dumbest tasks is to identify the paragraphs in the body, if we analyse the documents, there are different patterns of alignment of paragraphs. But most of the paragraphs are centre aligned. Now we need to figure out a way to extract the title. But before we get all pumped up and start coding, let us analyse the dataset little deep.\"\n",
    "document3 = \"Now one of the dumbest tasks is to guess the paragraphs in the body, if we guess the documents, there are different guesses of alignment of paragraphs. But most of the paragraphs are centre aligned. Now we need to guess out a way to guess the title. But before we get all guessed up and start guessing, let us guess the dataset little deep.\"\n",
    "\n",
    "documents = [document1,document2,document3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDocList = []\n",
    "\n",
    "# remove stop words, make all characters lower-case, and convert into a TextBlob object\n",
    "for doc in documents:\n",
    "    newDocList.append(tb(removeStopWords(doc).lower()))\n",
    "\n",
    "tfidfList = []\n",
    "scoreList = []\n",
    "    \n",
    "# calculate tf-idf for each month's messages\n",
    "bloblist = newDocList\n",
    "for i, blob in enumerate(bloblist):\n",
    "    \n",
    "    scores = {word: tfidf(word, blob, bloblist) for word in blob.words}\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    scores = []\n",
    "    words = []\n",
    "    \n",
    "    # could also skip this and just append sorted_words to the tfidfList and delete the scoreList. I just did this to keep words and scores seperate\n",
    "    for word, score in sorted_words[:3]: # the number here is determining how many of the top tf-idf words you want added to the list\n",
    "        scores.append(score)\n",
    "        words.append(word)\n",
    "        \n",
    "    tfidfList.append(words)\n",
    "    scoreList.append(scores)\n",
    "        \n",
    "data = pd.DataFrame({'document':documents,'docCleaned': newDocList,'tf-idf':tfidfList,'score':scoreList})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>docCleaned</th>\n",
       "      <th>tf-idf</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Now one of the important tasks is to identify ...</td>\n",
       "      <td>(n, o, w,  , o, n, e,  , i, m, p, o, r, t, a, ...</td>\n",
       "      <td>[important, titles, identify]</td>\n",
       "      <td>[0.012286821457823163, 0.012286821457823163, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Now one of the dumbest tasks is to identify th...</td>\n",
       "      <td>(n, o, w,  , o, n, e,  , d, u, m, b, e, s, t, ...</td>\n",
       "      <td>[dumbest, identify, paragraphs]</td>\n",
       "      <td>[0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Now one of the dumbest tasks is to guess the p...</td>\n",
       "      <td>(n, o, w,  , o, n, e,  , d, u, m, b, e, s, t, ...</td>\n",
       "      <td>[guess, guesses, guessed]</td>\n",
       "      <td>[0.061434107289115816, 0.012286821457823163, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document  \\\n",
       "0  Now one of the important tasks is to identify ...   \n",
       "1  Now one of the dumbest tasks is to identify th...   \n",
       "2  Now one of the dumbest tasks is to guess the p...   \n",
       "\n",
       "                                          docCleaned  \\\n",
       "0  (n, o, w,  , o, n, e,  , i, m, p, o, r, t, a, ...   \n",
       "1  (n, o, w,  , o, n, e,  , d, u, m, b, e, s, t, ...   \n",
       "2  (n, o, w,  , o, n, e,  , d, u, m, b, e, s, t, ...   \n",
       "\n",
       "                            tf-idf  \\\n",
       "0    [important, titles, identify]   \n",
       "1  [dumbest, identify, paragraphs]   \n",
       "2        [guess, guesses, guessed]   \n",
       "\n",
       "                                               score  \n",
       "0  [0.012286821457823163, 0.012286821457823163, 0.0]  \n",
       "1                                    [0.0, 0.0, 0.0]  \n",
       "2  [0.061434107289115816, 0.012286821457823163, 0...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
